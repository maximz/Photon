In statistical hypothesis testing the "p"-value is the probability of obtaining a test statistic at least as extreme as the one that was actually observed, assuming that the null hypothesis is true. One often "rejects the null hypothesis" when the "p"-value is less than the predetermined significance level α (Greek alpha), which is often 0.05 or 0.01, indicating that the observed result would be highly unlikely under the null hypothesis. Although there is often confusion, the p-value is not the probability of the null hypothesis being true, nor is the p-value the same as the Type I error rate.
Statistical hypothesis tests making use of p-values are commonly used in many fields of science, such as psychology and biology. Many common statistical tests, such as Student's t-test, produce test statistics which can be interpreted using p-values.
Coin flipping example.
As an example of a statistical test, an experiment is performed to determine whether a coin flip is fair (equal chance of landing heads or tails) or unfairly biased (one outcome being more likely than the other). 
Suppose that the experimental results show the coin turning up heads 14 times out of 20 total flips. The null hypothesis is that the coin is fair, so the p-value of this result is the chance of a fair coin landing on heads "at least" 14 times out of 20 flips. This probability can be computed from binomial coefficients as
This probability is the "p"-value, considering only extreme results which favor heads. This is called a one-tailed test. However, the deviation can be in either direction, favoring either heads or tails. We may instead calculate the two-tailed p-value, which considers deviations favoring either heads or tails. As the binomial distribution is symmetrical for a fair coin, the two-sided "p"-value is simply twice the above calculated single-sided "p"-value; "i.e.", the two-sided "p"-value is 0.115.
The calculated "p"-value exceeds 0.05, so the observation is consistent with the null hypothesis, as it falls within the range of what would happen 95% of the time were the coin in fact fair. Hence, we fail to reject the null hypothesis at the 5% level. Although the coin did not fall evenly, the deviation from expected outcome is small enough to be consistent with chance.
However, had one more head been obtained, the resulting "p"-value (two-tailed) would have been 0.0414 (4.14%). This time the null hypothesis – that the observed result of 15 heads out of 20 flips can be ascribed to chance alone – is rejected when using a 5% cut-off.
Interpretation.
Hypothesis tests, such as Student's t-test, typically produce test statistics whose sampling distributions under the null hypothesis are known. For instance, in the above coin-flipping example, the test statistic is the number of heads produced; this number follows a known binomial distribution if the coin is fair, and so the probability of any particular combination of heads and tails can be computed. To compute a p-value from the test statistic, one must simply sum (or integrate over) the probabilities of more extreme events occurring. For commonly used statistical tests, test statistics and their corresponding p-values are often tabulated in textbooks and reference works.
Traditionally, one rejects the null hypothesis if the "p"-value is less than or equal to the significance level, often represented by the Greek letter α (alpha). (Greek α is also used to indicate Type I error; the connection is that a hypothesis test that rejects the null hypothesis for all samples that have a p-value less than α will have a Type I error of α.) A significance level of 0.05 would deem extraordinary any result that is within the most extreme 5% of all possible results under the null hypothesis. In this case a p-value less than 0.05 would result in the rejection of the null hypothesis at the 5% (significance) level.
It is also important to consider the statistical power of a hypothesis test when interpreting its results. A test's power is the probability of correctly rejecting the null hypothesis when it is false; a test's power is influenced by the choice of significance level for the test, the size of the effect being measured, and the amount of data available. A hypothesis test may fail to reject the null, for example, if a true difference exists between two populations being compared by a t-test but the effect is small and the sample size is too small to distinguish the effect from random chance. Many clinical trials, for instance, have low statistical power to detect differences in adverse effects of treatments, since such effects are rare and the number of affected patients is very small.
Misunderstandings.
The data obtained by comparing the "p"-value to a significance level will yield one of two results: either the null hypothesis is rejected, or the null hypothesis "cannot" be rejected at that significance level (which however does not imply that the null hypothesis is "true"). A small "p"-value that indicates statistical significance does not indicate that an alternative hypothesis is necessarily correct.
Despite the ubiquity of "p"-value tests, this particular test for statistical significance has come under heavy criticism due both to its inherent shortcomings and the potential for misinterpretation.
There are several common misunderstandings about "p"-values.
Criticisms.
Critics of p-values point out that the criterion used to decide "statistical significance" is based on the somewhat arbitrary choice of level (often set at 0.05). If significance testing is applied to hypotheses that are known to be false in advance, a non-significant result will simply reflect an insufficient sample size; a p-value depends only on the information obtained from a given experiment.
The definition of "more extreme" data depends on the sampling methodology adopted by the investigator; for example, the situation in which the investigator flips the coin 100 times yielding 50 heads has a set of extreme data that is different from the situation in which the investigator continues to flip the coin until 50 heads are achieved yielding 100 flips. This is to be expected, as the experiments are different experiments, and the sample spaces and the probability distributions for the outcomes are different even though the observed data (50 heads out of 100 flips) are the same for the two experiments. 
Other problems are sometimes also claimed to exist in association with p-values. First, although a study or research is only conducted once, p-values are sometimes interpreted in terms of long-term frequency, which to some does not make perfect sense; however it is specifically the probability that a single outcome of the test-statistic will exceed a given value which just happens to coincide with an observed value. Second, some say that p-values can be impacted by possibilities that never actually occurred, as if this were bad. Third, only by changing the way the hypothesis testing question is asked, it would be highly possible to get different P-values from exactly the same data; but that is to be expected, as the answer to a different question will typically be different.
Some regard the p-value "p" as the main result of statistical significance testing, rather than the acceptance or rejection of the null hypothesis at a pre-prescribed significance level. Fisher proposed "p" as an informal measure of evidence against the null hypothesis. He called on researchers to combine "p" in the mind with other types of evidence for and against that hypothesis, such as the a priori plausibility of the hypothesis and the relative strengths of results from previous studies. Many misunderstandings concerning "p" arise because statistics classes and instructional materials ignore or at least do not emphasize the role of prior evidence in interpreting "p". A renewed emphasis on prior evidence could encourage researchers to place "p" in the proper context, evaluating a hypothesis by weighing "p" together with all the other evidence about the hypothesis.
Related quantities.
A closely related concept is the E-value, which is the average number of times in multiple testing that one expects to obtain a test statistic at least as extreme as the one that was actually observed, assuming that the null hypothesis is true. The E-value is the product of the number of tests and the p-value.
The 'inflated' (or adjusted) p-value, is when a group of p-values are changed according to some multiple comparisons procedure so that each of the adjusted p-values can now be compared to the same threshold level of significance (formula_2), while keeping the type I error controlled. The control is in the sense that the specific procedures controls it, it might be controlling the familywise error rate, the false discovery rate, or some other error rate.
