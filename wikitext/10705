\,e^{ -\frac{(x-\mu)^2}{2\sigma^2} }</math>
In probability theory, the normal (or Gaussian) distribution is a continuous probability distribution, defined by the formula
The parameter "μ" in this formula is the "mean" or "expectation" of the distribution (and also its median and mode). The parameter "σ" is its standard deviation; its variance is therefore . A random variable with a Gaussian distribution is said to be normally distributed and is called a normal deviate.
If and , the distribution is called the standard normal distribution or the unit normal distribution, and a random variable with that distribution is a standard normal deviate.
Normal distributions are extremely important in statistics, and are often used in the natural and social sciences for real-valued random variables whose distributions are not known. One reason for their popularity is the central limit theorem, which states that, under mild conditions, the mean of a large number of random variables independently drawn from the same distribution is distributed approximately normally, irrespective of the form of the original distribution. Thus, physical quantities that are expected to be the sum of many independent processes (such as measurement errors) often have a distribution very close to normal. Another reason is that a large number of results and methods (such as propagation of uncertainty and least squares parameter fitting) can be derived analytically, in explicit form, when the relevant variables are normally distributed. 
The normal distribution is also the only absolutely continuous distribution all of whose cumulants beyond the first two (i.e. other than the mean and variance) are zero. It is also the continuous distribution with the maximum entropy for a given mean and variance.
The normal distribution is symmetric about its mean, and is non-zero over the entire real line. As such it may not be a suitable model for variables that are inherently positive or strongly skewed, such as the weight of a person or the price of a share. Such variables may be better described by other distributions, such as the log-normal distribution or the Pareto distribution.
The normal distribution is also practically zero once the value "x" lies more than a few standard deviations away from the mean. Therefore, it may not be appropriate when one expects a significant fraction of outliers, values that lie many standard deviations away from the mean. Least-squares and other statistical inference methods which are optimal for normally distributed variables often become highly unreliable. In those cases, one assumes a more heavy-tailed distribution, and the appropriate robust statistical inference methods.
The normal distributions are a subclass of the elliptical distributions.
The Gaussian distribution is sometimes informally called the bell curve. However, there are many other distributions that are bell-shaped (such as Cauchy's, Student's, and logistic). The terms Gaussian function and Gaussian bell curve are also ambiguous since they sometimes refer to multiples of the normal distribution whose integral is not 1; that is, formula_2 for arbitrary positive constants "a", "b" and "c".
Definition.
Standard normal distribution.
The factor formula_4 in this expression ensures that the total area under the curve "ϕ"("x") is equal to one]. The in the exponent ensures that the distribution has unit variance (and therefore also unit standard deviation). This function is symmetric around "x"=0, where it attains its maximum value formula_5; and has inflection points at +1 and −1.
General normal distribution.
Any normal distribution is a version of the standard normal distribution whose domain has been stretched by a factor "σ" (the standard deviation) and then translated by "μ" (the mean value), that is 
The probability density must be scaled by formula_7 so that the integral is still 1. 
If "Z" is a standard normal deviate, then "X" = "μ" + "σZ" will have a normal distribution with expected value "μ" and standard deviation "σ". Conversely, if "X" is a general normal deviate, then "Z" = ("X" − "μ")/"σ" will have a standard normal distribution.
where "a" is negative and "c" is formula_9. In this form, the mean value "μ" is −"b"/"a", and the variance "σ"2 is −1/(2"a"). For the standard normal distribution, "a" is −1/2, "b" is zero, and "c" is formula_10.
Notation.
The standard Gaussian distribution (with zero mean and unit variance) is often denoted with the Greek letter "ϕ" (phi). The alternative form of the Greek phi letter, "φ", is also used quite often.
The normal distribution is also often denoted by "N"("μ", "σ"2). Thus when a random variable "X" is distributed normally with mean "μ" and variance "σ"2, we write
Alternative parametrizations.
Some authors advocate using the precision "τ" as the parameter defining the width of the distribution, instead of the deviation "σ" or the variance "σ"2. The precision is normally defined as the reciprocal of the variance, 1/"σ"2. The formula for the distribution then becomes
This choice is claimed to have advantages in numerical computations when "σ" is very close to zero, and to simplify formulas in some contexts, such as in the Bayesian inference of variables with multivariate normal distribution.
Occasionally, the precision "τ" is defined as 1/"σ", the reciprocal of the standard deviation; so that 
Alternative definitions.
Authors may differ also on which normal distribution should be called the "standard" one. Gauss himself defined the standard normal as having variance , that is
According to Stigler, this formulation is advantageous because of a much simpler and easier-to-remember formula, the fact that the pdf has unit height at zero, and simple approximate formulas for the quantiles of the distribution.
Properties.
Moments.
The plain and absolute moments of a variable "X" are the expected values of "X""p" and |"X"|"p",respectively. If the expected value "μ" of "X" is zero, these parameters are called "central moments". Usually we are interested only in moments with integer order "p".
If "X" has a normal distribution, these moments exist and are finite for any "p" whose real part is greater than −1. For any non-negative integer "p", the plain central moments are
The cumulant generating function is the logarithm of the moment generating function, namely
Since this is a quadratic polynomial in "t", only the first two cumulants are nonzero, namely the mean "μ" and the variance "σ"2.
Cumulative distribution.
The cumulative distribution function (CDF) of a random variable is the probability of its value falling in the interval formula_18, as a function of "x". The CDF of the standard normal distribution, usually denoted with the capital Greek letter formula_19 (phi), is the integral
In statistics one often uses the related error function, or erf("x"), defined as the probability of a random variable with normal distribution of mean 0 and variance 1/2 falling in the range formula_21; that is
These integrals cannot be expressed in terms of elementary functions, and are often said to be special functions. They are closely related, namely
For a generic normal distribution "f" with mean "μ" and deviation "σ", the cumulative distribution function is
The complement of the standard normal CDF, formula_25, is often called the Q-function, especially in engineering texts. It gives the probability of that the value of a standard normal random variable "X" will exceed "x". Other definitions of the "Q"-function, all of which are simple transformations of formula_19, are also used occasionally.
The graph of the standard normal CDF formula_19 has 2-fold rotational symmetry around the point (0,1/2); that is, formula_28. Its antiderivative (indefinite integral) formula_29 is formula_30.
Standard deviation and tolerance intervals.
About 68% of values drawn from a normal distribution are within one standard deviation "σ" away from the mean; about 95% of the values lie within two standard deviations; and about 99.7% are within three standard deviations. This fact is known as the 68-95-99.7 (empirical) rule, or the "3-sigma rule".
More precisely, the probability that a normal deviate lies in the range and is given by
Zero-variance limit.
In the limit when "σ" tends to zero, the probability density "f"("x") eventually tends to zero at any , but grows without limit if , while its integral remains equal to 1. Therefore, the normal distribution cannot be defined as an ordinary function when .
However, one can define the normal distribution with zero variance as a generalized function; specifically, as Dirac's "delta function" "δ" translated by the mean "μ", that is "f"("x") = "δ"("x"−"μ").
Its CDF is then the Heaviside step function translated by the mean "μ", namely
Then, as "n" increases, the probability distribution of "Z" will
tend to the normal distribution with zero mean and variance "σ"2.
The theorem can be extended to variables "Xi" are not independent and/or not identically distributed, if certain constraints on the degree of dependence and the moments
of the distributions.
A great number of test statistics, scores, and estimators encountered in practice contain sums of certain random variables in them, even more estimators can be represented as sums of random variables through the use of influence functions. The central limit theorem implies that those statistical parameters will have asymptotically normal distributions.
Whether these approximations are sufficiently accurate depends on the purpose for which they are needed, and the rate of convergence to the normal distribution. It is typically the case that such approximations are less accurate in the tails of the distribution.
A general upper bound for the approximation error in the central limit theorem is given by the Berry–Esseen theorem, improvements of the approximation are given by the Edgeworth expansions.
Operations on normal deviates.
The family of normal distributions is closed under linear transformations. That is, if "X" is normally distributed with mean "μ" and deviation "σ", then the variable , for any real numbers "a" and "b", is also normally distributed, with 
mean "aμ" + "b" and deviation "aσ".
Also if "X"1 and "X"2 are two independent normal random variables, with means "μ"1, "μ"2 and standard deviations "σ"1, "σ"2, then their sum will also be normally distributed,] with mean "μ"1 + "μ"2 and variance formula_34.
In particular, if "X" and "Y" are independent normal deviates with zero mean and variance "σ"2, then and are also independent and normally distributed, with zero mean and variance 2"σ"2. This is a special case of the polarization identity.
Also, if "X"1, "X"2 are two independent normal deviates with mean "μ" and deviation "σ", and "a", "b" are arbitrary real numbers, then the variable
Operations on the density function.
The split normal distribution is most directly defined in terms of joining scaled sections of the density functions of different normal distributions and rescaling the density to integrate to one. The truncated normal distribution results from rescaling a section of a single density function.
Extensions.
The notion of normal distribution, being one of the most important distributions in probability theory, has been extended far beyond the standard framework of the univariate (that is one-dimensional) case (Case 1). All these extensions are also called "normal" or "Gaussian" laws, so a certain ambiguity in names exists.
Estimation of parameters.
In particular, both estimators are asymptotically efficient for "σ"2.
where "tk,p" and are the "p"th quantiles of the "t"- and "χ"2-distributions respectively. These confidence intervals are of the "level" , meaning that the true values "μ" and "σ"2 fall outside of these intervals with probability "α". In practice people usually take , resulting in the 95% confidence intervals. The approximate formulas in the display above were derived from the asymptotic distributions of formula_41 and "s"2. The approximate formulas become valid for large values of "n", and are more convenient for the manual calculation since the standard normal quantiles "z""α"/2 do not depend on "n". In particular, the most popular value of , results in .
Bayesian analysis of the normal distribution.
The formulas for the non-linear-regression cases are summarized in the conjugate prior article.
The sum of two quadratics.
Scalar form.
The following auxiliary formula is useful for simplifying the posterior update equations, which otherwise become fairly tedious.
Vector form.
A similar formula can be written for the sum of two vector quadratics: If x, y, z are vectors of length "k", and A and B are symmetric, invertible matrices of size formula_62, then
where
In other words, it sums up all possible combinations of products of pairs of elements from x, with a separate coefficient for each. In addition, since formula_66, only the sum formula_67 matters for any off-diagonal elements of A, and there is no loss of generality in assuming that A is symmetric. Furthermore, if A is symmetric, then the form formula_68 .
The sum of differences from the mean.
where formula_70
With known variance.
For a set of i.i.d. normally distributed data points X of size "n" where each individual point "x" follows formula_71 with known variance σ2, the conjugate prior distribution is also normally distributed.
This can be shown more easily by rewriting the variance as the precision, i.e. using τ = 1/σ2. Then if formula_72 and formula_73 we proceed as follows.
In the above derivation, we used the formula above for the sum of two quadratics and eliminated all constant factors not involving μ. The result is the kernel of a normal distribution, with mean formula_76 and precision formula_77, i.e.
That is, to combine "n" data points with total precision of "n"τ (or equivalently, total variance of "n"/σ2) and mean of values formula_80, derive a new total precision simply by adding the total precision of the data to the prior total precision, and form a new mean through a "precision-weighted average", i.e. a weighted average of the data mean and the prior mean, each weighted by the associated total precision. This makes logical sense if the precision is thought of as indicating the certainty of the observations: In the distribution of the posterior mean, each of the input components is weighted by its certainty, and the certainty of this distribution is the sum of the individual certainties. (For the intuition of this, compare the expression "the whole is (or is not) greater than the sum of its parts". In addition, consider that the knowledge of the posterior comes from a combination of the knowledge of the prior and likelihood, so it makes sense that we are more certain of it than of either of its components.)
The above formula reveals why it is more convenient to do Bayesian analysis of conjugate priors for the normal distribution in terms of the precision. The posterior precision is simply the sum of the prior and likelihood precisions, and the posterior mean is computed through a precision-weighted average, as described above. The same formulas can be written in terms of variance by reciprocating all the precisions, yielding the more ugly formulas
With known mean.
where 
This is also a scaled inverse chi-squared distribution, where
or equivalently
With unknown mean and variance.
For a set of i.i.d. normally distributed data points X of size "n" where each individual point "x" follows formula_71 with unknown mean μ and variance σ2, a combined (multivariate) conjugate prior is placed over the mean and variance, consisting of a normal-inverse-gamma distribution.
The respective numbers of pseudo-observations just add the number of actual observations to them. The new mean hyperparameter is once again a weighted average, this time weighted by the relative numbers of observations. Finally, the update for formula_93 is similar to the case with known mean, but in this case the sum of squared deviations is taken with respect to the observed data mean rather than the true mean, and as a result a new "interaction term" needs to be added to take care of the additional error source stemming from the deviation between prior and data mean.
Proof is as follows.
p(\mu|\sigma^2; \mu_0, n_0) &\sim \mathcal{N}(\mu_0,\sigma_0^2/n_0) = \frac{1}{\sqrt{2\pi\frac{\sigma^2}{n_0}}} \exp\left(-\frac{n_0}{2\sigma^2}(\mu-\mu_0)^2\right) \\
&\propto (\sigma^2)^{-1/2} \exp\left(-\frac{n_0}{2\sigma^2}(\mu-\mu_0)^2\right) \\
p(\sigma^2; \nu_0,\sigma_0^2) &\sim I\chi^2(\nu_0,\sigma_0^2) = IG(\nu_0/2, \nu_0\sigma_0^2/2) \\
&= \frac{(\sigma_0^2\nu_0/2)^{\nu_0/2}}{\Gamma(\nu_0/2)}~\frac{\exp\left[ \frac{-\nu_0 \sigma_0^2}{2 \sigma^2}\right]}{(\sigma^2)^{1+\nu_0/2}} \\
&\propto {(\sigma^2)^{-(1+\nu_0/2)}} \exp\left[ \frac{-\nu_0 \sigma_0^2}{2 \sigma^2}\right] 
\end{align}</math>
Therefore, the joint prior is
where formula_96
In other words, the posterior distribution has the form of a product of a normal distribution over "p"(μ|σ2) times an inverse gamma distribution over "p"(σ2), with parameters that are the same as the update equations above.
Occurrence.
Approximate normality.
"Approximately" normal distributions occur in many situations, as explained by the central limit theorem. When the outcome is produced by a large number of small effects acting "additively and independently", its distribution will be close to normal. The normal approximation will not be valid if the effects act multiplicatively (instead of additively), or if there is a single external influence which has a considerably larger magnitude than the rest of the effects.
Assumed normality.
There are statistical methods to empirically test that assumption, see the above Normality tests section.
Generating values from normal distribution.
In computer simulations, especially in applications of the Monte-Carlo method, it is often desirable to generate values that are normally distributed. The algorithms listed below all generate the standard normal deviates, since a can be generated as , where "Z" is standard normal. All these algorithms rely on the availability of a random number generator "U" capable of producing uniform random variates.
−1("U") will have the standard normal distribution. The drawback of this method is that it relies on calculation of the probit function Φ−1, which cannot be done analytically. Some approximate methods are described in and in the erf article. Wichura gives a fast algorithm for computing this function to 16 decimal places, which is used by R to compute random variates of the normal distribution.
<li>An easy to program approximate approach, that relies on the central limit theorem, is as follows: generate 12 uniform "U"(0,1) deviates, add them all up, and subtract 6 – the resulting random variable will have approximately standard normal distribution. In truth, the distribution will be Irwin–Hall, which is a 12-section eleventh-order polynomial approximation to the normal distribution. This random deviate will have a limited range of (−6, 6).
<li>The Box–Muller method uses two independent random numbers "U" and "V" distributed uniformly on (0,1). Then the two random variables "X" and "Y"
formula_100 
where "h" is "the measure of the precision of the observations". Using this normal law as a generic model for errors in the experiments, Gauss formulates what is now known as the non-linear weighted least squares (NWLS) method.
Although Gauss was the first to suggest the normal distribution law, Laplace made significant contributions. It was Laplace who first posed the problem of aggregating several observations in 1774, although his own solution led to the Laplacian distribution. It was Laplace who first calculated the value of the integral in 1782, providing the normalization constant for the normal distribution. Finally, it was Laplace who in 1810 proved and presented to the Academy the fundamental central limit theorem, which emphasized the theoretical importance of the normal distribution.
It is of interest to note that in 1809 an American mathematician Adrain published two derivations of the normal probability law, simultaneously and independently from Gauss. His works remained largely unnoticed by the scientific community, until in 1871 they were "rediscovered" by Abbe.
In the middle of the 19th century Maxwell demonstrated that the normal distribution is not just a convenient mathematical tool, but may also occur in natural phenomena: "The number of particles whose velocity, resolved in a certain direction, lies between "x" and "x" + "dx" is
The term "standard normal" which denotes the normal distribution with zero mean and unit variance came into general use around 1950s, appearing in the popular textbooks by P.G. Hoel (1947) ""Introduction to mathematical statistics"" and A.M. Mood (1950) ""Introduction to the theory of statistics"".
When the name is used, the "Gaussian distribution" was named after Carl Friedrich Gauss, who introduced the distribution in 1809 as a way of rationalizing the method of least squares as outlined above. Among English speakers, both "normal distribution" and "Gaussian distribution" are in common use, with different terms preferred by different communities.
