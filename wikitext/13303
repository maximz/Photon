In probability theory and statistics, the Poisson distribution (pronounced ) is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time and/or space if these events occur with a known average rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.
For instance, suppose someone typically gets 4 pieces of mail per day on average. There will be, however, a certain spread: sometimes a little more, sometimes a little less, once in a while nothing at all. Given only the average rate, for a certain period of observation (pieces of mail per day, phonecalls per hour, etc.), and assuming that the process, or mix of processes, that produces the event flow is essentially random, the Poisson distribution specifies how likely it is that the count will be 3, or 5, or 11, or any other number, during one period of observation. That is, it predicts the degree of spread around a known average rate of occurrence.
The Derivation of the Poisson distribution section shows the relation with a formal definition.
Historical background of the Poisson distribution was described by Gullberg (1997).
History.
The distribution was first introduced by Siméon Denis Poisson (1781–1840) and published, together with his probability theory, in 1837 in his work "Recherches sur la probabilité des jugements en matière criminelle et en matière civile" (“Research on the Probability of Judgments in Criminal and Civil Matters”). The work focused on certain random variables "N" that count, among other things, the number of discrete occurrences (sometimes called “arrivals”) that take place during a time-interval of given length. The result had been given previously by Abraham de Moivre (1711) in "De Mensura Sortis seu; de Probabilitate Eventuum in Ludis a Casu Fortuito Pendentibus" in Philosophical Transactions of the Royal Society, p. 219.
A practical application of this distribution was made by Ladislaus Bortkiewicz in 1898 when he was given the task of investigating the number of soldiers in the Prussian army killed accidentally by horse kick; this experiment introduced the Poisson distribution to the field of reliability engineering.
Definition.
where
The positive real number "λ" is equal to the expected value of "X" and also to its variance
The Poisson distribution can be applied to systems with a large number of possible events, each of which is rare. The Poisson distribution is sometimes called a Poissonian.
Occurrence.
Derivation of Poisson distribution — The "law of rare events".
The Poisson distribution may be derived by considering an interval, in time, space or otherwise, in which events happen at random with a known average number formula_43. The interval is divided in formula_44 subintervals formula_45 of equal size. The probability that an event will fall in the subinterval formula_46 is for each formula_47 equal to formula_48, and the occurrence of an event in formula_46 may be approximatively considered to be a Bernoulli trial. The total number formula_50 of events then will be approximatively binomial distributed with parameters formula_44 and formula_52 The approximation will be better with increasing formula_44, and the formula_54-distribution converges to the Poisson distribution with parameter formula_55
In several of the above examples—such as, the number of mutations in a given sequence of DNA—the events being counted are actually the outcomes of discrete trials, and would more precisely be modelled using the binomial distribution, that is
In such cases "n" is very large and "p" is very small (and so the expectation "np" is of intermediate magnitude). Then the distribution may be approximated by the less cumbersome Poisson distribution
This approximation is sometimes known as the law of rare events, since each of the "n" individual Bernoulli events rarely occurs. The name may be misleading because the total count of success events in a Poisson process need not be rare if the parameter "np" is not small. For example, the number of telephone calls to a busy switchboard in one hour follows a Poisson distribution with the events appearing frequent to the operator, but they are rare from the point of view of the average member of the population who is very unlikely to make a call to that switchboard in that hour.
The word law is sometimes used as a synonym of probability distribution, and "convergence in law" means "convergence in distribution". Accordingly, the Poisson distribution is sometimes called the law of small numbers because it is the probability distribution of the number of occurrences of an event that happens rarely but has very many opportunities to happen. "The Law of Small Numbers" is a book by Ladislaus Bortkiewicz about the Poisson distribution, published in 1898. Some have suggested that the Poisson distribution should have been called the Bortkiewicz distribution.
Multi-dimensional Poisson process.
The poisson distribution arises as the distribution of counts of occurrences of events in (multidimensional) intervals in multidimensional Poisson processes in a directly equivalent way to the result for unidimensional processes. This,is "D" is any region the multidimensional space for which |D|, the area or volume of the region, is finite, and if is count of the number of events in "D", then
Other applications in science.
In a Poisson process, the number of observed occurrences fluctuates about its mean "λ" with a standard deviation formula_59. These fluctuations are denoted as Poisson noise or (particularly in electronics) as "shot noise".
The correlation of the mean and standard deviation in counting independent discrete occurrences is useful scientifically. By monitoring how the fluctuations vary with the mean signal, one can estimate the contribution of a single occurrence, "even if that contribution is too small to be detected directly". For example, the charge "e" on an electron can be estimated by correlating the magnitude of an electric current with its shot noise. If "N" electrons pass a point in a given time "t" on the average, the mean current is formula_60; since the current fluctuations should be of the order formula_61 (i.e., the standard deviation of the Poisson process), the charge formula_62 can be estimated from the ratio formula_63.
An everyday example is the graininess that appears as photographs are enlarged; the graininess is due to Poisson fluctuations in the number of reduced silver grains, not to the individual grains themselves. By correlating the graininess with the degree of enlargement, one can estimate the contribution of an individual grain (which is otherwise too small to be seen unaided). Many other molecular applications of Poisson noise have been developed, e.g., estimating the number density of receptor molecules in a cell membrane.
Since each observation has expectation λ so does this sample mean. Therefore the maximum likelihood estimate is an unbiased estimator of λ. It is also an efficient estimator, i.e. its estimation variance achieves the Cramér–Rao lower bound (CRLB). Hence it is MVUE. Also it can be proved that the sum (and hence the sample mean as it is a one-to-one function of the sum) is a complete and sufficient statistic for λ.
To prove sufficiency we may use the factorization theorem. Consider partitioning the probability mass function of the joint Poisson distribution for the sample into two parts: one which depends solely on the sample formula_65 (called formula_66) and one which depends on the parameter formula_67 and the sample formula_65 only through the function formula_69. Then, formula_69 is a sufficient statistic for formula_67.
Note that the first term, formula_66, depends only on formula_65. The second term, formula_75, depends on the sample only through formula_76. Thus, formula_69 is sufficient.
For completeness, a family of distributions is said to be complete if and only if formula_78 implies that formula_79 for all formula_67. If the individual formula_81 are iid formula_82, then formula_83. Knowing the distribution we want to investigate it is easy to see that the statistic is complete.
For this equality to hold, it is obvious that formula_85 must be 0. This follows from the fact that none of the other terms will be 0 for all formula_86 in the sum and for all possible values of formula_67. Hence, formula_78 for all formula_67 implies that formula_79 and the statistic has been shown to be complete.
Confidence interval.
where "k" is the number of event occurrences in a given interval and formula_92 is the chi-square deviate with lower tail area "p" and degrees of freedom "n". This interval is 'exact' in the sense that its coverage probability is never less than the nominal .
where "k" is the number of event occurrences in a given interval and formula_94 is the p-quantile of a Gamma distribution with shape parameter n and scale parameter 1. This interval is 'exact' in the sense that its coverage probability is never less than the nominal . Note that the quantiles of the Gamma distribution may be calculated using the quantiles of the chi-square distribution.
where formula_96 denotes the standard normal deviate with upper tail area .
For application of these formulae in the same context as above (given a sample of "n" measured values "k""i"), one would set
calculate an interval for "μ=nλ", and then derive the interval for "λ".
Bayesian inference.
In Bayesian inference, the conjugate prior for the rate parameter "λ" of the Poisson distribution is the gamma distribution. Let
Then, given the same sample of "n" measured values "k""i" as before, and a prior of Gamma("α", "β"), the posterior distribution is
The posterior mean E["λ"] approaches the maximum likelihood estimate formula_101 in the limit as formula_102.
The posterior predictive distribution for a single additional observation is a negative binomial distribution distribution, sometimes called a Gamma–Poisson distribution.
Bivariate Poisson distribution.
This distribution has been extended to the bivariate case. The generating function for this distribution is
with
The marginal distributions are Poisson("θ"1) and Poisson("θ"2) and the correlation coefficient is limited to the range
The Skellam distribution is a particular case of this distribution. 
Poisson distribution and prime numbers.
Gallagher in 1976 showed that prime numbers in short intervals obey a "Poisson distribution". Ernie Croot (2010) stated this in informal mathematical language in his lecture notes on the Poisson distribution. To understand this relationship the Prime Number Theorem will be required. 
This theorem states that the number of primes formula_106 is about formula_107, where the logarithm is take to the base "e". In symbols if formula_108 denotes the number of primes less than "x" then
This implies that 
In what follows the notation formula_106 is used to denote the number of primes in a given interval formula_112. 
Suppose that formula_113 is a "large" number, say formula_114. Then a number chosen at random formula_115 has formula_116 (~ 0.43%) chance that it will be prime. A typical interval formula_117 will contain about one prime.
Notice that equality was not used here: in order to obtain equality we would have to let formula_124 in some fashion. The larger formula_113 is the closer the above probability comes to formula_126. 
It is an interesting exercise to determine out why the primes would be expected to be "Poisson-distributed".
